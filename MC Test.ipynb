{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import geom, multivariate_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_sample = 1024\n",
    "dim_x = 20\n",
    "dim_z = 20\n",
    "r = 0.6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_theta_x(theta):\n",
    "    mu = theta\n",
    "    covariance = 2 * np.eye(dim_x)\n",
    "    return multivariate_normal(mean=mu, cov=covariance)\n",
    "\n",
    "def p_theta_x_given_z(z):\n",
    "    mu = z\n",
    "    covariance = np.eye(dim_x)\n",
    "    return multivariate_normal(mean=mu, cov=covariance)\n",
    "\n",
    "def p_theta_z(theta):\n",
    "    mu = theta\n",
    "    covariance = np.eye(dim_z)\n",
    "    return multivariate_normal(mean=mu, cov=covariance)\n",
    "\n",
    "p = geom\n",
    "\n",
    "def logmeanexp(data, axis=None):\n",
    "    max_val = np.max(data, axis=axis)\n",
    "    return max_val + np.log(np.mean(np.exp(data - max_val), axis=axis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulation des observations x : on fixe theta_0 qui sera estimé par la suite. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.9273549   0.33482018  3.57706978  4.13042656  6.18264001  5.53791592\n",
      "   4.77281956  8.03746989  8.87192774 10.73869364  7.80488678 13.60427996\n",
      "  15.35014703 13.93989203 13.97866803 17.27583844 17.36332741 17.21515519\n",
      "  20.94081452 19.30262564]\n",
      " [-0.49211995 -0.28160759  2.33667874  3.38763304  3.73595128  6.00995612\n",
      "   5.19611569  9.70996431  8.91294655 10.52639965 11.88312733 13.9241664\n",
      "  10.90936035 14.04677807 14.04933701 16.65732746 17.98330548 16.77511653\n",
      "  19.64482665 20.53698236]]\n",
      "-35.169274755844725\n"
     ]
    }
   ],
   "source": [
    "theta_0 = (1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20)\n",
    "X = p_theta_x(theta_0).rvs(size=N_sample)\n",
    "print(X[:2])\n",
    "\n",
    "l_true = np.mean(np.log(p_theta_x(theta_0).pdf(X)))\n",
    "print(l_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = 1 / 2 * np.eye(dim_z, dim_x) #size = dim_z, dim_x\n",
    "b = np.mean(X, axis=0) #size = dim_z, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_phi_z_given_x(x):\n",
    "    mu = A @ x + b\n",
    "    covariance = 2 / 3 * np.eye(dim_z)\n",
    "    return multivariate_normal(mean=mu, cov=covariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'l_hat_ml_ss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/onyxia/work/MC Test.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://user-ecassant-199994-0.user.lab.sspcloud.fr/home/onyxia/work/MC%20Test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m empirical_bias_squared_ss \u001b[39m=\u001b[39m (l_hat_ml_ss \u001b[39m-\u001b[39m l_true) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://user-ecassant-199994-0.user.lab.sspcloud.fr/home/onyxia/work/MC%20Test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mcarré du biais empirique de l\u001b[39m\u001b[39m'\u001b[39m\u001b[39mestimateur ss :\u001b[39m\u001b[39m\"\u001b[39m, empirical_bias_squared_ss)\n\u001b[1;32m      <a href='vscode-notebook-cell://user-ecassant-199994-0.user.lab.sspcloud.fr/home/onyxia/work/MC%20Test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m empirical_bias_squared_rr \u001b[39m=\u001b[39m (l_hat_ml_rr \u001b[39m-\u001b[39m l_true) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'l_hat_ml_ss' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "empirical_bias_squared_ss = (l_hat_ml_ss - l_true) ** 2\n",
    "print(\"carré du biais empirique de l'estimateur ss :\", empirical_bias_squared_ss)\n",
    "\n",
    "empirical_bias_squared_rr = (l_hat_ml_rr - l_true) ** 2\n",
    "print(\"carré du biais empirique de l'estimateur rr :\", empirical_bias_squared_rr)\n",
    "\n",
    "empirical_bias_squared_iwae = (l_hat_iwae - l_true) ** 2\n",
    "print(\"carré du biais empirique de l'estimateur iwae :\", empirical_bias_squared_iwae)\n",
    "\n",
    "empirical_bias_squared_sumo = (l_hat_sumo - l_true) ** 2\n",
    "print(\"carré du biais empirique de l'estimateur sumo :\", empirical_bias_squared_sumo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition de l'estimateur \n",
    "def estimateur_ml_ss(theta, x):\n",
    "    K = p.rvs(r)\n",
    "    size = 2 ** (K + 1)\n",
    "    Z = q_phi_z_given_x(x).rvs(size=size)\n",
    "    Z_E, Z_O = Z[::2], Z[1::2]\n",
    "    log_weights = [np.log(p_theta_z(theta).pdf(z) * p_theta_x_given_z(z).pdf(x) / q_phi_z_given_x(x).pdf(z)) for z in Z]\n",
    "    log_weights_E, log_weights_O = log_weights[::2], log_weights[1::2]\n",
    "    I_0 = np.mean(log_weights)\n",
    "    l_hat_E, l_hat_O = logmeanexp(log_weights_E), logmeanexp(log_weights_O)\n",
    "    l_hat_O_E = logmeanexp(log_weights)\n",
    "    delta_K = l_hat_O_E - 0.5 * (l_hat_O + l_hat_E)\n",
    "    l_hat_ml_ss_x = I_0 + delta_K / p(r).pmf(K)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Retourner l'estimation\n",
    "    return l_hat_ml_ss_x\n",
    "\n",
    "def aggregate_estimator_ml_ss(theta):\n",
    "    l_hat_ml_ss_list = []\n",
    "\n",
    "    for x in X:\n",
    "        l_hat_ml_ss_list.append(estimateur_ml_ss(theta, x))\n",
    "    return np.mean(l_hat_ml_ss_list)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_95902/1571981411.py:7: RuntimeWarning: divide by zero encountered in log\n",
      "  log_weights = [np.log(p_theta_z(theta).pdf(z) * p_theta_x_given_z(z).pdf(x) / q_phi_z_given_x(x).pdf(z)) for z in Z]\n",
      "/tmp/ipykernel_95902/217849312.py:20: RuntimeWarning: invalid value encountered in subtract\n",
      "  return max_val + np.log(np.mean(np.exp(data - max_val), axis=axis))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "print(aggregate_estimator_ml_ss((1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_95902/1571981411.py:7: RuntimeWarning: divide by zero encountered in log\n",
      "  log_weights = [np.log(p_theta_z(theta).pdf(z) * p_theta_x_given_z(z).pdf(x) / q_phi_z_given_x(x).pdf(z)) for z in Z]\n",
      "/tmp/ipykernel_95902/217849312.py:20: RuntimeWarning: invalid value encountered in subtract\n",
      "  return max_val + np.log(np.mean(np.exp(data - max_val), axis=axis))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta optimal trouvé par l'optimisation: [ 2.  3.  2.  5.  5.  7.  6.  8.  9. 10.  9. 13. 13. 13. 15. 16. 16. 18.\n",
      " 17. 20.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Définir une fonction objectif pour l'optimisation\n",
    "def objectif_ml_ss(theta):\n",
    "    return -aggregate_estimator_ml_ss(theta)  # On minimise l'opposé de l'estimateur\n",
    "\n",
    "# Supposer une valeur initiale pour theta\n",
    "theta_init_ml_ss = [2, 3, 2, 5, 5, 7, 6, 8, 9, 10, 9, 13, 13, 13, 15, 16, 16, 18, 17, 20]\n",
    "\n",
    "# Minimiser la fonction objectif pour trouver theta optimal\n",
    "resultat_optimisation_ml_ss = minimize(objectif_ml_ss, theta_init_ml_ss)\n",
    "\n",
    "# Récupérer le theta optimal trouvé par l'optimisation\n",
    "theta_optimal_ml_ss = resultat_optimisation_ml_ss.x\n",
    "\n",
    "# Imprimer le résultat\n",
    "print(\"Theta optimal trouvé par l'optimisation:\", theta_optimal_ml_ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test avec ml_rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimateur_ml_rr(theta, x):\n",
    "    K = p.rvs(r)\n",
    "    size = 2 ** (K + 1)\n",
    "    Z = q_phi_z_given_x(x).rvs(size=size)\n",
    "    Z_E, Z_O = Z[::2], Z[1::2]\n",
    "    log_weights = [np.log(p_theta_z(theta).pdf(z) * p_theta_x_given_z(z).pdf(x) / q_phi_z_given_x(x).pdf(z)) for z in Z]\n",
    "    log_weights_E, log_weights_O = log_weights[::2], log_weights[1::2]\n",
    "    I_0 = np.mean(log_weights)\n",
    "    l_hat_E, l_hat_O = logmeanexp(log_weights_E), logmeanexp(log_weights_O)\n",
    "    l_hat_O_E = logmeanexp(log_weights)\n",
    "    delta_k_list_rr = [logmeanexp(log_weights[:2**(k+1)]) - 0.5 * (logmeanexp(log_weights_O[:2**k]) + logmeanexp(log_weights_E[:2**k])) for k in range(K+1)]\n",
    "    l_hat_ml_rr_x = I_0 + np.sum([delta_k_list_rr[k] / (1 - p(r).cdf(k-1)) for k in range(K+1)])\n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Retourner l'estimation\n",
    "    return l_hat_ml_rr_x\n",
    "\n",
    "def aggregate_estimator_ml_rr(theta):\n",
    "    l_hat_ml_rr_list = []\n",
    "\n",
    "    for x in X:\n",
    "        l_hat_ml_rr_list.append(estimateur_ml_rr(theta, x))\n",
    "    return np.mean(l_hat_ml_rr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_95902/2570684156.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  log_weights = [np.log(p_theta_z(theta).pdf(z) * p_theta_x_given_z(z).pdf(x) / q_phi_z_given_x(x).pdf(z)) for z in Z]\n",
      "/tmp/ipykernel_95902/217849312.py:20: RuntimeWarning: invalid value encountered in subtract\n",
      "  return max_val + np.log(np.mean(np.exp(data - max_val), axis=axis))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta optimal trouvé par l'optimisation: [ 1.06345614 -0.35839561  1.76785886  1.66490607  0.14678718 -0.10568308\n",
      " -1.41893123  1.07329356 -0.30465404  0.72262995  0.55217586 -0.2002613\n",
      "  0.01808234  1.11868775 -0.133515   -0.33666163  0.27683162  0.22678559\n",
      "  0.02206721 -0.9667478 ]\n"
     ]
    }
   ],
   "source": [
    "# Définir une fonction objectif pour l'optimisation\n",
    "def objectif_ml_rr(theta):\n",
    "    return -aggregate_estimator_ml_rr(theta)  # On minimise l'opposé de l'estimateur\n",
    "\n",
    "# Supposer une valeur initiale pour theta\n",
    "theta_init_ml_rr = np.random.randn(20)\n",
    "\n",
    "# Minimiser la fonction objectif pour trouver theta optimal\n",
    "resultat_optimisation_ml_rr = minimize(objectif_ml_rr, theta_init_ml_rr)\n",
    "\n",
    "# Récupérer le theta optimal trouvé par l'optimisation\n",
    "theta_optimal_ml_rr = resultat_optimisation_ml_rr.x\n",
    "\n",
    "# Imprimer le résultat\n",
    "print(\"Theta optimal trouvé par l'optimisation:\", theta_optimal_ml_rr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test avec iwae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimateur_iwae(theta, x):\n",
    "    K = p.rvs(r)\n",
    "    size = 2 ** (K + 1)\n",
    "    Z = q_phi_z_given_x(x).rvs(size=size)\n",
    "    Z_E, Z_O = Z[::2], Z[1::2]\n",
    "    log_weights = [np.log(p_theta_z(theta).pdf(z) * p_theta_x_given_z(z).pdf(x) / q_phi_z_given_x(x).pdf(z)) for z in Z]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Retourner l'estimation\n",
    "    return logmeanexp(log_weights)\n",
    "\n",
    "def aggregate_estimator_iwae(theta):\n",
    "    l_hat_iwae_list = []\n",
    "\n",
    "    for x in X:\n",
    "        l_hat_iwae_list.append(estimateur_iwae(theta, x))\n",
    "    return np.mean(l_hat_iwae_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_95902/2167204708.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  log_weights = [np.log(p_theta_z(theta).pdf(z) * p_theta_x_given_z(z).pdf(x) / q_phi_z_given_x(x).pdf(z)) for z in Z]\n",
      "/tmp/ipykernel_95902/217849312.py:20: RuntimeWarning: invalid value encountered in subtract\n",
      "  return max_val + np.log(np.mean(np.exp(data - max_val), axis=axis))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta optimal trouvé par l'optimisation: [ 0.99066879 -0.12741301  1.93299073  0.65851622 -1.26168042 -0.37484456\n",
      "  1.51298273 -1.39810348  1.03646724 -0.22213902  0.30820356  0.29121353\n",
      " -0.47940173  0.03726505  2.79478743  1.11376067  0.48151411  0.03756264\n",
      " -0.79826138 -0.78188891]\n"
     ]
    }
   ],
   "source": [
    "# Définir une fonction objectif pour l'optimisation\n",
    "def objectif_iwae(theta):\n",
    "    return -aggregate_estimator_iwae(theta)  # On minimise l'opposé de l'estimateur\n",
    "\n",
    "# Supposer une valeur initiale pour theta\n",
    "theta_init_iwae = np.random.randn(20)\n",
    "\n",
    "# Minimiser la fonction objectif pour trouver theta optimal\n",
    "resultat_optimisation_iwae = minimize(objectif_iwae, theta_init_iwae)\n",
    "\n",
    "# Récupérer le theta optimal trouvé par l'optimisation\n",
    "theta_optimal_iwae = resultat_optimisation_iwae.x\n",
    "\n",
    "# Imprimer le résultat\n",
    "print(\"Theta optimal trouvé par l'optimisation:\", theta_optimal_iwae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test avec Sumo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimateur_sumo(theta, x):\n",
    "    K = p.rvs(r)\n",
    "    size = 2 ** (K + 1)\n",
    "    Z = q_phi_z_given_x(x).rvs(size=size)\n",
    "    Z_E, Z_O = Z[::2], Z[1::2]\n",
    "    log_weights = [np.log(p_theta_z(theta).pdf(z) * p_theta_x_given_z(z).pdf(x) / q_phi_z_given_x(x).pdf(z)) for z in Z]\n",
    "    log_weights_E, log_weights_O = log_weights[::2], log_weights[1::2]\n",
    "    I_0 = np.mean(log_weights)\n",
    "    delta_k_list_sumo = [logmeanexp(log_weights[:2**(k+1)]) - logmeanexp(log_weights[:2**k]) for k in range(K+1)]\n",
    "    l_hat_sumo_x = I_0 + np.sum([delta_k_list_sumo[k] / (1 - p(r).cdf(k-1)) for k in range(K+1)])\n",
    "    \n",
    "    \n",
    "\n",
    "    return l_hat_sumo_x\n",
    "\n",
    "def aggregate_estimator_sumo(theta):\n",
    "    l_hat_sumo_list = []\n",
    "\n",
    "    for x in X:\n",
    "        l_hat_sumo_list.append(estimateur_sumo(theta, x))\n",
    "    return np.mean(l_hat_sumo_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_95902/77341551.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  log_weights = [np.log(p_theta_z(theta).pdf(z) * p_theta_x_given_z(z).pdf(x) / q_phi_z_given_x(x).pdf(z)) for z in Z]\n",
      "/tmp/ipykernel_95902/217849312.py:20: RuntimeWarning: invalid value encountered in subtract\n",
      "  return max_val + np.log(np.mean(np.exp(data - max_val), axis=axis))\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'l_hat_sumo_list_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/onyxia/work/MC Test.ipynb Cell 20\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://user-ecassant-199994-0.user.lab.sspcloud.fr/home/onyxia/work/MC%20Test.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m theta_init_sumo \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandn(\u001b[39m20\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://user-ecassant-199994-0.user.lab.sspcloud.fr/home/onyxia/work/MC%20Test.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Minimiser la fonction objectif pour trouver theta optimal\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://user-ecassant-199994-0.user.lab.sspcloud.fr/home/onyxia/work/MC%20Test.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m resultat_optimisation_sumo \u001b[39m=\u001b[39m minimize(objectif_sumo, theta_init_sumo)\n\u001b[1;32m     <a href='vscode-notebook-cell://user-ecassant-199994-0.user.lab.sspcloud.fr/home/onyxia/work/MC%20Test.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Récupérer le theta optimal trouvé par l'optimisation\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://user-ecassant-199994-0.user.lab.sspcloud.fr/home/onyxia/work/MC%20Test.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m theta_optimal_sumo \u001b[39m=\u001b[39m resultat_optimisation_sumo\u001b[39m.\u001b[39mx\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/scipy/optimize/_minimize.py:708\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    706\u001b[0m     res \u001b[39m=\u001b[39m _minimize_cg(fun, x0, args, jac, callback, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[1;32m    707\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbfgs\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 708\u001b[0m     res \u001b[39m=\u001b[39m _minimize_bfgs(fun, x0, args, jac, callback, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions)\n\u001b[1;32m    709\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnewton-cg\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    710\u001b[0m     res \u001b[39m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    711\u001b[0m                              \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/scipy/optimize/_optimize.py:1372\u001b[0m, in \u001b[0;36m_minimize_bfgs\u001b[0;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, finite_diff_rel_step, xrtol, c1, c2, hess_inv0, **unknown_options)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[39mif\u001b[39;00m maxiter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1370\u001b[0m     maxiter \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(x0) \u001b[39m*\u001b[39m \u001b[39m200\u001b[39m\n\u001b[0;32m-> 1372\u001b[0m sf \u001b[39m=\u001b[39m _prepare_scalar_function(fun, x0, jac, args\u001b[39m=\u001b[39;49margs, epsilon\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m   1373\u001b[0m                               finite_diff_rel_step\u001b[39m=\u001b[39;49mfinite_diff_rel_step)\n\u001b[1;32m   1375\u001b[0m f \u001b[39m=\u001b[39m sf\u001b[39m.\u001b[39mfun\n\u001b[1;32m   1376\u001b[0m myfprime \u001b[39m=\u001b[39m sf\u001b[39m.\u001b[39mgrad\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/scipy/optimize/_optimize.py:288\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[0;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[1;32m    284\u001b[0m     bounds \u001b[39m=\u001b[39m (\u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39minf, np\u001b[39m.\u001b[39minf)\n\u001b[1;32m    286\u001b[0m \u001b[39m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[39m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m sf \u001b[39m=\u001b[39m ScalarFunction(fun, x0, args, grad, hess,\n\u001b[1;32m    289\u001b[0m                     finite_diff_rel_step, bounds, epsilon\u001b[39m=\u001b[39;49mepsilon)\n\u001b[1;32m    291\u001b[0m \u001b[39mreturn\u001b[39;00m sf\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:166\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[0;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39m=\u001b[39m fun_wrapped(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx)\n\u001b[1;32m    165\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_fun_impl \u001b[39m=\u001b[39m update_fun\n\u001b[0;32m--> 166\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun()\n\u001b[1;32m    168\u001b[0m \u001b[39m# Gradient evaluation\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(grad):\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:262\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_fun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    261\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated:\n\u001b[0;32m--> 262\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun_impl()\n\u001b[1;32m    263\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:163\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_fun\u001b[39m():\n\u001b[0;32m--> 163\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39m=\u001b[39m fun_wrapped(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:145\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnfev \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    142\u001b[0m \u001b[39m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[39m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m fx \u001b[39m=\u001b[39m fun(np\u001b[39m.\u001b[39;49mcopy(x), \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    146\u001b[0m \u001b[39m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misscalar(fx):\n",
      "\u001b[1;32m/home/onyxia/work/MC Test.ipynb Cell 20\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://user-ecassant-199994-0.user.lab.sspcloud.fr/home/onyxia/work/MC%20Test.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mobjectif_sumo\u001b[39m(theta):\n\u001b[0;32m----> <a href='vscode-notebook-cell://user-ecassant-199994-0.user.lab.sspcloud.fr/home/onyxia/work/MC%20Test.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39maggregate_estimator_sumo(theta)\n",
      "\u001b[1;32m/home/onyxia/work/MC Test.ipynb Cell 20\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://user-ecassant-199994-0.user.lab.sspcloud.fr/home/onyxia/work/MC%20Test.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m l_hat_sumo_list \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell://user-ecassant-199994-0.user.lab.sspcloud.fr/home/onyxia/work/MC%20Test.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m X:\n\u001b[0;32m---> <a href='vscode-notebook-cell://user-ecassant-199994-0.user.lab.sspcloud.fr/home/onyxia/work/MC%20Test.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     l_hat_sumo_list\u001b[39m.\u001b[39mappend(estimateur_sumo(theta, x))\n\u001b[1;32m     <a href='vscode-notebook-cell://user-ecassant-199994-0.user.lab.sspcloud.fr/home/onyxia/work/MC%20Test.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean(l_hat_sumo_list)\n",
      "\u001b[1;32m/home/onyxia/work/MC Test.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://user-ecassant-199994-0.user.lab.sspcloud.fr/home/onyxia/work/MC%20Test.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m delta_k_list_sumo \u001b[39m=\u001b[39m [logmeanexp(log_weights[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)]) \u001b[39m-\u001b[39m logmeanexp(log_weights[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mk]) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(K\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)]\n\u001b[1;32m     <a href='vscode-notebook-cell://user-ecassant-199994-0.user.lab.sspcloud.fr/home/onyxia/work/MC%20Test.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m l_hat_sumo_x \u001b[39m=\u001b[39m I_0 \u001b[39m+\u001b[39m np\u001b[39m.\u001b[39msum([delta_k_list_sumo[k] \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m p(r)\u001b[39m.\u001b[39mcdf(k\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(K\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)])\n\u001b[0;32m---> <a href='vscode-notebook-cell://user-ecassant-199994-0.user.lab.sspcloud.fr/home/onyxia/work/MC%20Test.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m l_hat_sumo_list_x\u001b[39m.\u001b[39mappend(l_hat_sumo_x)\n\u001b[1;32m     <a href='vscode-notebook-cell://user-ecassant-199994-0.user.lab.sspcloud.fr/home/onyxia/work/MC%20Test.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean(l_hat_sumo_list_x)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'l_hat_sumo_list_x' is not defined"
     ]
    }
   ],
   "source": [
    "# Définir une fonction objectif pour l'optimisation\n",
    "def objectif_sumo(theta):\n",
    "    return -aggregate_estimator_sumo(theta)  # On minimise l'opposé de l'estimateur\n",
    "\n",
    "# Supposer une valeur initiale pour theta\n",
    "theta_init_sumo = np.random.randn(20)\n",
    "\n",
    "# Minimiser la fonction objectif pour trouver theta optimal\n",
    "resultat_optimisation_sumo = minimize(objectif_sumo, theta_init_sumo)\n",
    "\n",
    "# Récupérer le theta optimal trouvé par l'optimisation\n",
    "theta_optimal_sumo = resultat_optimisation_sumo.x\n",
    "\n",
    "# Imprimer le résultat\n",
    "print(\"Theta optimal trouvé par l'optimisation:\", theta_optimal_sumo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
